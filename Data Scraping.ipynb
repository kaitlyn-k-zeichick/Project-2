{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the web\n",
    "url = 'https://www.mountainproject.com/route/106683500/quo-vadis'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check to make sure the url was found. 200 means success.\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the html code into a string\n",
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse using BeautifulSoup\n",
    "soup = BeautifulSoup(page, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quo Vadis'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find route's name\n",
    "def route_name(soup):\n",
    "    name = soup.find('h1').text\n",
    "    name = name.split()\n",
    "    name = ' '.join(name)\n",
    "    return name\n",
    "\n",
    "route_name(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find route's rating\n",
    "def rating_value(soup):\n",
    "    runtime_regex = re.compile('starsWithAvgText')\n",
    "    rating = soup.find(id=runtime_regex).text\n",
    "    rating = rating.split('from')[0]\n",
    "    rating = rating.split(':')[1]\n",
    "    rating = rating.split()\n",
    "    rating = float(rating[0])\n",
    "    return rating\n",
    "\n",
    "rating_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of votes\n",
    "def vote_value(soup):\n",
    "    runtime_regex = re.compile('starsWithAvgText')\n",
    "    votes = soup.find(id=runtime_regex).text\n",
    "    votes = votes.split('from')[1]\n",
    "    votes = votes.split()[0].split()[0].replace(',','')\n",
    "    votes = int(votes)\n",
    "    return votes\n",
    "\n",
    "vote_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.9+'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find route's grade. It's a string to take V grades into account.\n",
    "def grade_value(soup):\n",
    "    if soup.find(class_='rateYDS') == None:\n",
    "        grade = None\n",
    "        return grade\n",
    "    grade = soup.find(class_='rateYDS').text.split()[0]\n",
    "    grade = str(grade)\n",
    "    return grade\n",
    "\n",
    "grade_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Trad', 'Aid']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find climbing type\n",
    "def type_value(soup):\n",
    "    list_types = []\n",
    "    type_climb = str(soup.find(text='Type:').findNext())\n",
    "    type_climb = type_climb.split() #new\n",
    "    for element in type_climb: #new\n",
    "        element = element.replace(',','') #new\n",
    "        if (element=='Trad') or (element=='Sport') or (element=='TR') or (element=='Boulder') or (element=='Aid') or (element=='Ice') or (element=='Snow') or (element=='Alpine'):\n",
    "            list_types.append(element)\n",
    "    #type_climb = type_climb.split()[1].replace(',', '')\n",
    "    return list_types\n",
    "\n",
    "type_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find route's height in feet\n",
    "def height_value(soup):\n",
    "    height = str(soup.find(text='Type:').findNext()).split()\n",
    "    if len(height) <= 3:\n",
    "        height = None\n",
    "    elif 'ft' not in height:\n",
    "        height = None\n",
    "    else:\n",
    "        height = int(str(soup.find(text='Type:').findNext()).split('ft')[0].split()[-1])\n",
    "    return height\n",
    "    \n",
    "height_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of pitches\n",
    "def pitches(soup):\n",
    "    num_pitches = str(soup.find(text='Type:').findNext()).split()\n",
    "    if ('pitches,' not in num_pitches) and ('pitches' not in num_pitches):\n",
    "        num_pitches = 1\n",
    "    else:\n",
    "        num_pitches = int(str(soup.find(text='Type:').findNext()).split('pitches')[0].split()[-1])\n",
    "    return num_pitches\n",
    "\n",
    "pitches(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Safety Rating\n",
    "def safe_value(soup):\n",
    "    safety = soup.find(class_=\"inline-block mr-2\").text.split()\n",
    "    safety = safety[-1].split()[0]\n",
    "    if (safety != 'R') and (safety != 'PG13') and (safety != 'G') and (safety != 'X'): \n",
    "        return None\n",
    "    return safety\n",
    "\n",
    "safe_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VI'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find Commitment Rating\n",
    "def commitment_value(soup):\n",
    "    NCCS = str(soup.find(text='Type:').findNext()).split()\n",
    "    #if len(NCCS) <= 9:\n",
    "    #    NCCS = None\n",
    "    if 'Grade' not in NCCS:\n",
    "        NCCS = None\n",
    "    else:\n",
    "        NCCS = str(NCCS).split('Grade')[1]\n",
    "        NCCS = NCCS.split()[1].replace(',', '')\n",
    "        NCCS = NCCS[1:-1]\n",
    "    return NCCS\n",
    "\n",
    "commitment_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'California'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find State\n",
    "def state_value(soup):\n",
    "    area = soup.find(class_=\"mb-half small text-warm\").text.split('>')\n",
    "    area_list = []\n",
    "    for element in area:\n",
    "        area_list.append(element.split())\n",
    "    area_list = area_list[1:]\n",
    "    area_list = area_list[0]\n",
    "    area_list = ' '.join(area_list)\n",
    "    return area_list\n",
    "\n",
    "state_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B. El Capitan'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find smallest sub-area\n",
    "def sub_area_value(soup):\n",
    "    area = soup.find(class_=\"mb-half small text-warm\").text.split('>')\n",
    "    area_list = []\n",
    "    for element in area:\n",
    "        area_list.append(element.split())\n",
    "    area_list = area_list[1:]\n",
    "    area_list = area_list[-2]\n",
    "    area_list = ' '.join(area_list)\n",
    "    #area_list = area_list[0]\n",
    "    return area_list\n",
    "\n",
    "sub_area_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of photos\n",
    "def photo_value(soup):\n",
    "    photos = 0\n",
    "    for a in soup.find_all('a', class_='card-with-photo photo-card'):\n",
    "        for link in a.find_all(class_=\"img-container position-relative\"):\n",
    "            photos +=1\n",
    "    if soup.find(id=\"more-photos-button\"):#If there are more photos not displayed\n",
    "            more_photos = soup.find(id=\"more-photos-button\").text\n",
    "            more_photos = more_photos.split('More')[0]\n",
    "            more_photos = int(more_photos.split()[-1])\n",
    "            photos = photos + more_photos\n",
    "    return photos\n",
    "    \n",
    "photo_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find number of comments\n",
    "def comment_value(soup):\n",
    "    comments = soup.find(class_='comment-count').text\n",
    "    comments = int(comments.split()[0])\n",
    "    return comments\n",
    "\n",
    "comment_value(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.8',\n",
       " '5.10',\n",
       " '5.11',\n",
       " '5.13a',\n",
       " '5.13a',\n",
       " '5.9',\n",
       " '5.13b',\n",
       " 'V10',\n",
       " '5.7',\n",
       " '5.14a',\n",
       " '5.9',\n",
       " '5.13',\n",
       " '5.10',\n",
       " '5.9',\n",
       " '5.9',\n",
       " '5.9',\n",
       " '5.8',\n",
       " '5.9',\n",
       " '5.9',\n",
       " '5.11c',\n",
       " '5.10+']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find list of grades of nearby routes\n",
    "def nearby_grades(soup):\n",
    "    list_nearby_grades = []\n",
    "    runtime_regex = re.compile('max-height max-height-md-0 max-height-xs-400')\n",
    "    near_grades = soup.find_all(class_=runtime_regex)\n",
    "    for element in soup.find_all(class_=runtime_regex):\n",
    "        for grade in element.find_all(class_='rateYDS'):\n",
    "            list_nearby_grades.append(grade.text)\n",
    "    this_grade = grade_value(soup)\n",
    "    if this_grade in list_nearby_grades:\n",
    "        list_nearby_grades.remove(this_grade)\n",
    "    return list_nearby_grades\n",
    "\n",
    "nearby_grades(soup)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A4+'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find aid grade\n",
    "def aid_value(soup):\n",
    "    aid = soup.find(class_=\"inline-block mr-2\").text.split()\n",
    "    if len(aid) < 2:\n",
    "        aid = None\n",
    "        return aid\n",
    "    aid1 = aid[-2]\n",
    "    aid1_first_letter = aid[-2][0]\n",
    "    aid2 = aid[-1]\n",
    "    aid2_first_letter = aid[-1][0]\n",
    "    if (aid1_first_letter == 'A'): \n",
    "        aid = aid1\n",
    "    elif (aid2_first_letter == 'A'):\n",
    "        aid = aid2\n",
    "    else:\n",
    "        aid = None\n",
    "    return aid\n",
    "\n",
    "aid_value(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single function for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Quo Vadis',\n",
       "  'rating': 4.0,\n",
       "  'votes': 1,\n",
       "  'grade': '5.9+',\n",
       "  'type': ['Trad', 'Aid'],\n",
       "  'height': 3000,\n",
       "  'pitches': 22,\n",
       "  'safety': None,\n",
       "  'commitment': 'VI',\n",
       "  'state': 'California',\n",
       "  'sub_area': 'B. El Capitan',\n",
       "  'photos': 4,\n",
       "  'comments': 3,\n",
       "  'near_grades': ['5.8',\n",
       "   '5.10',\n",
       "   '5.11',\n",
       "   '5.13a',\n",
       "   '5.13a',\n",
       "   '5.9',\n",
       "   '5.13b',\n",
       "   'V10',\n",
       "   '5.7',\n",
       "   '5.14a',\n",
       "   '5.9',\n",
       "   '5.13',\n",
       "   '5.10',\n",
       "   '5.9',\n",
       "   '5.9',\n",
       "   '5.9',\n",
       "   '5.8',\n",
       "   '5.9',\n",
       "   '5.9',\n",
       "   '5.11c',\n",
       "   '5.10+'],\n",
       "  'aid_grade': 'A4+'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scrape_features(link):\n",
    "    \"\"\"\"\n",
    "    Takes in a link and extracts various features from the MP page.\n",
    "    Returns a dictionary of the features.\n",
    "    \"\"\"\n",
    "    #Scrape the web\n",
    "    url = link\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    #Turn the html code into a string\n",
    "    page = response.text\n",
    "    \n",
    "    #Parse using BeautifulSoup\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    #Extract features\n",
    "    name = route_name(soup)\n",
    "    rating = rating_value(soup)\n",
    "    votes = vote_value(soup)\n",
    "    grade = grade_value(soup)\n",
    "    type_climb = type_value(soup)\n",
    "    height = height_value(soup)\n",
    "    number_pitches = pitches(soup)\n",
    "    safety = safe_value(soup)\n",
    "    commitment = commitment_value(soup)\n",
    "    state = state_value(soup)\n",
    "    sub_area = sub_area_value(soup)\n",
    "    num_photos = photo_value(soup)\n",
    "    num_comments = comment_value(soup)\n",
    "    neighbor_grades = nearby_grades(soup)\n",
    "    aid_grade = aid_value(soup)\n",
    "    \n",
    "    #Create list with a dictionary of features\n",
    "    headers = ['name', 'rating', 'votes', 'grade', 'type', 'height', 'pitches', 'safety', 'commitment',\n",
    "          'state', 'sub_area', 'photos', 'comments', 'near_grades', 'aid_grade']\n",
    "\n",
    "    mp_data = []\n",
    "    mp_dict = dict(zip(headers, [name,\n",
    "                                rating,\n",
    "                                votes,\n",
    "                                grade,\n",
    "                                type_climb,\n",
    "                                height,\n",
    "                                number_pitches,\n",
    "                                safety,\n",
    "                                commitment,\n",
    "                                state,\n",
    "                                sub_area,\n",
    "                                num_photos,\n",
    "                                num_comments,\n",
    "                                neighbor_grades,\n",
    "                                aid_grade]))\n",
    "\n",
    "    mp_data.append(mp_dict)\n",
    "    \n",
    "    return mp_data\n",
    "\n",
    "scrape_features('https://www.mountainproject.com/route/106683500/quo-vadis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all data in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jtree. Type: Boulder\n",
    "url = 'https://www.mountainproject.com/route-finder?selectedIds=105720495&type=boulder&diffMinrock=800&diffMinboulder=20000&diffMinaid=74500&diffMinice=30000&diffMinmixed=50000&diffMaxrock=12400&diffMaxboulder=21700&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=0&pitches=0&sort1=area&sort2=rating'\n",
    "\n",
    "response = requests.get(url)\n",
    "page = response.text\n",
    "\n",
    "soup2 = BeautifulSoup(page,\"lxml\")\n",
    "\n",
    "soup2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of links to the routes\n",
    "list_links = []\n",
    "for tr in soup2.find_all('tr', class_='route-row'):\n",
    "    for link in tr.find_all('a')[::4]:\n",
    "        list_links.append(link.get(\"href\"))\n",
    "list_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of routes and their features\n",
    "link_dict = {}\n",
    "count = 0\n",
    "\n",
    "for link in list_links[1:250]:\n",
    "    print(count, ': ', link)\n",
    "    features = scrape_features(link) #features is a list with dictionary\n",
    "    features = features[0] #features is a dictionary\n",
    "    link_dict[features['name']] = features\n",
    "    count = count + 1\n",
    "\n",
    "print(link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn into data frame\n",
    "california_routes_info = pd.DataFrame(link_dict).T\n",
    "california_routes_info.set_index('name', inplace=True)\n",
    "\n",
    "california_routes_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single function to input list of links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "\n",
    "def scrape_links_all(url, new_filename):\n",
    "    #Put into proper Beautiful Soup formal\n",
    "    ua = UserAgent()\n",
    "    user_agent = {'User-agent': ua.random}\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    page = response.text\n",
    "    soup2 = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "    #List of links to the routes\n",
    "    list_links = []\n",
    "    for tr in soup2.find_all('tr', class_='route-row'):\n",
    "        for link in tr.find_all('a')[::4]:\n",
    "            list_links.append(link.get(\"href\"))\n",
    "            \n",
    "    #Dictionary of routes and their features    \n",
    "    link_dict = {}\n",
    "    count = 0\n",
    "    for link in list_links[:1000]:\n",
    "        print(\"count: \", count, \"link: \", link)\n",
    "        features = scrape_features(link) #features is a list with dictionary\n",
    "        features = features[0] #features is a dictionary\n",
    "        link_dict[features['name']] = features\n",
    "        time.sleep(.5+2*random.random())\n",
    "        count = count + 1\n",
    "\n",
    "        \n",
    "    #Turn into data frame\n",
    "    california_routes_info = pd.DataFrame(link_dict).T\n",
    "    california_routes_info.set_index('name', inplace=True)\n",
    "    \n",
    "    #Save it as a csv file\n",
    "    california_routes_info.to_csv(new_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yosemite\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105833381&sort1=area&sort2=rating&stars=0&type=rock&viewAll=1', 'yosemite_rock.csv')\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105833381&sort1=area&sort2=rating&stars=0&type=boulder&viewAll=1', 'yosemite_boulder.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red Rocks \n",
    "#Rocks\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105731932&sort1=area&sort2=rating&stars=0&type=rock&viewAll=1', 'redrocks_rock.csv')\n",
    "#Boulders\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105731932&sort1=area&sort2=rating&stars=0&type=boulder&viewAll=1', 'redrocks_boulder.csv')\n",
    "\n",
    "#California Aid Routes\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105708959&sort1=area&sort2=rating&stars=0&type=aid&viewAll=1', 'ca_aid.csv')\n",
    "\n",
    "#Sierra East\n",
    "#Boulders\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=74500&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105798288&sort1=area&sort2=rating&stars=0&type=boulder&viewAll=1', 'sierra_east_boulder.csv')\n",
    "#Rocks\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=74500&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105798288&sort1=area&sort2=rating&stars=0&type=rock&viewAll=1', 'sierra_east_rock.csv')\n",
    "\n",
    "#Joshua Tree\n",
    "#Boulders\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=74500&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105720495&sort1=area&sort2=rating&stars=0&type=boulder&viewAll=1', 'jtree_boulder.csv')\n",
    "#Rocks\n",
    "scrape_links_all('https://www.mountainproject.com/route-finder?diffMaxaid=75260&diffMaxboulder=21700&diffMaxice=38500&diffMaxmixed=60000&diffMaxrock=12400&diffMinaid=70000&diffMinboulder=20000&diffMinice=30000&diffMinmixed=50000&diffMinrock=800&is_sport_climb=1&is_top_rope=1&is_trad_climb=1&pitches=0&selectedIds=105720495&sort1=area&sort2=rating&stars=0&type=rock&viewAll=1', 'jtree_rock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
